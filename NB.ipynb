{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"><a href=http://localhost:8002/tree/test3/demo.ipynb?token=2914855d8b4521cfd506bc5dc2daf28ea7b6139d2984325e target=\"_blank\">Edit</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "data = pd.read_csv('.data/Data-WikipediaComments/train.csv')\n",
    "COMMENT = 'comment_text'\n",
    "data[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "train, test = train_test_split(data, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# vectorize data\n",
    "n = train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )\n",
    "trn_term_doc = vec.fit_transform(train[COMMENT])\n",
    "tst_term_doc = vec.transform(test[COMMENT])\n",
    "\n",
    "# save vertorize files\n",
    "#with open('.artifacts/vectorizer.pickle', 'wb') as handle:\n",
    "#    pickle.dump(vec, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# run model\n",
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "x = trn_term_doc\n",
    "x_t= tst_term_doc\n",
    "\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r\n",
    "\n",
    "label_cols=['toxic']\n",
    "\n",
    "for i, j in enumerate(label_cols):\n",
    "    #print('fit', j)\n",
    "    m,r = get_mdl(train[j])\n",
    "    filename=\".artifacts/\"+j+\".pickle\"\n",
    "    #print(filename)\n",
    "    #pickle.dump(m, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publish results\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#results=[accuracy_score(train[\"toxic\"], m.predict(x)),accuracy_score(test[\"toxic\"], m.predict(x_t))]\n",
    "#results=pd.DataFrame(results)\n",
    "#results=(results.T)\n",
    "#results.columns=[\"Train Result\",\"Test Result\"]\n",
    "#results.to_csv(\".artifacts/result.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
